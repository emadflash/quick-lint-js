#!/usr/bin/env python3
import argparse
import re
import sys
from concurrent.futures import ThreadPoolExecutor as Pool
from urllib.parse import urldefrag, urljoin, urlparse

import requests
from bs4 import BeautifulSoup


allow_mails = ["strager.nds@gmail.com"]

allowed_file_ext_for_soup = [
    "html",
    "js",
]

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3"
}


def check_fragment(soup, fragment) -> bool:
    if fragment:
        return True if soup.select(f"#{fragment}") else False
    return True


def is_mailto_link(link) -> bool:
    return True if re.search(r"^mailto:*", link) else False


def get_mail_address(link) -> str:
    return link.split(":")[1]


class UrlPacket(object):
    def __init__(self, parent, url, defraged_url=None, fragment=None):
        self.parent = parent
        self.url = url
        self.defraged_url = defraged_url
        self.fragment = fragment


class UrlNotFound(Exception):
    """Raised when requests returns error code other than 200"""

    def __init__(self, response_code):
        self.response_code = response_code


def log(error, UrlPacket) -> None:
    print(f"({error}) {UrlPacket.parent}, {UrlPacket.url}")


def check_response(url):
    response = requests.head(url, headers=headers, allow_redirects=True)
    if not response.ok:
        raise UrlNotFound(response.status_code)
    return response.url


def print_if_url_broken(url_packet):
    try:
        check_response(url_packet.url)
    except UrlNotFound as e:
        log(e.response_code, url_packet)


class initialize_crawler:
    def __init__(self, site, allowed_file_ext_for_soup):
        self.site = site
        self.allowed_file_ext_for_soup = allowed_file_ext_for_soup
        self.site_namespace = urlparse(self.site).netloc
        self.visted_urls = list()
        self.visted_urls_soup = dict()
        self.external_links_to_check = list()

        try:
            result = requests.get(self.site)
            soup = BeautifulSoup(result.text, "html.parser")
            self.urls = self.get_urls_from_page(soup)
        except requests.exceptions.ConnectionError:
            print("(error) failed to get", self.site)
            exit(0)


class CheckExternalLinks(list):
    """list of UrlPacket"""

    def __init__(self, urls):
        super().__init__(urls)

    def check(self):
        with Pool(max_workers=len(self)) as executor:
            future_to_url = {
                executor.submit(print_if_url_broken, packet) for packet in self
            }
            for future in future_to_url:
                try:
                    data = future.result()
                except Exception as e:
                    pass


class Crawler(initialize_crawler):
    def __init__(self, site, allowed_file_ext_for_soup=allowed_file_ext_for_soup):
        super().__init__(site, allowed_file_ext_for_soup)

    def in_namespace(self, url) -> bool:
        return urlparse(url).netloc == self.site_namespace

    def get_urls_from_page(self, soup) -> list:
        href_tags = soup.find_all(href=True)
        hrefs = [tag.get("href") for tag in href_tags]
        return hrefs

    def in_allowed_file_soup(self, url) -> bool:
        return urlparse(url).path.split(".")[-1] in self.allowed_file_ext_for_soup

    def start_crawl(self):
        self.crawl_and_report(self.site, self.urls)
        CheckExternalLinks(self.external_links_to_check).check()

    def get_urls_to_crawl(self, packet) -> list:
        urls_from_page = list()
        if self.in_allowed_file_soup(packet.url):
            result = requests.get(packet.defraged_url)
            self.visted_urls_soup[packet.defraged_url] = BeautifulSoup(
                result.text, "html.parser"
            )
            if not check_fragment(
                self.visted_urls_soup[packet.defraged_url], packet.fragment
            ):
                print(
                    "(fragment missing)",
                    packet.parent_url,
                    packet.url,
                    packet.fragment,
                )
            urls_from_page = self.get_urls_from_page(
                self.visted_urls_soup[packet.defraged_url]
            )
        return urls_from_page

    def CheckInternalLinks(self, packet):
        try:
            _url = check_response(packet.defraged_url)
            urls_from_page = self.get_urls_to_crawl(packet)
            if len(urls_from_page) != 0:
                self.crawl_and_report(_url, urls_from_page)
        except UrlNotFound as e:
            print(f"({e.response_code} error) {packet.parent}, {packet.url}")

    def crawl_and_report(self, parent_url, urls):
        for link in urls:
            if is_mailto_link(link):
                mail_link = get_mail_address(link)
                if mail_link not in allow_mails:
                    print(f"(unknown mail) {parent_url}, {mail_link}")
            else:
                url = urljoin(parent_url, link)
                if url not in self.visted_urls:
                    self.visted_urls.append(url)
                    defraged_url, fragment = urldefrag(url)
                    if defraged_url not in self.visted_urls_soup:
                        if not self.in_namespace(defraged_url):
                            self.external_links_to_check.append(
                                UrlPacket(parent_url, url)
                            )
                            self.visted_urls_soup[defraged_url] = None
                        else:
                            self.CheckInternalLinks(
                                UrlPacket(parent_url, url, defraged_url, fragment)
                            )
                    else:
                        soup = self.visted_urls_soup[defraged_url]
                        if soup is not None and not check_fragment(soup, fragment):
                            print("(fragment missing)", parent_url, url, fragment)


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument("site", type=str)
    if len(sys.argv) == 1:
        parser.print_help()
        exit(1)
    args = parser.parse_args()
    crawler = Crawler(args.site)
    crawler.start_crawl()


if __name__ == "__main__":
    main()
